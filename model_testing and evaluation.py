# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UGSOWQMnVC1IAdcyTs0TJEgBnKn1hg_V
"""

def sequences_to_original(X, seq_length):
    original_shape = np.zeros((X.shape[0] + seq_length, ) + X.shape[2:])
    for i in range(X.shape[0]):
        original_shape[i] = X[i][0]
    return original_shape

def get_data_clusters(clustered_dataframes, index):
    return clustered_dataframes[index].values

def process_data(file_path, cluster_labels, T):
    data = load_data(file_path)
    clustered_columns = get_clusters(data, cluster_labels)
    data.columns = data.columns.astype(str)

    X = []
    for idx in range(len(cluster_labels)):
        cluster_data = data[clustered_columns[idx]].values
        x, _ = create_sequences(cluster_data, T)
        X.append(x)

    return X, data

def dynamic_thresholding(scored, window_size=60, threshold_percentile=90):
    anomaly_results = []
    # Handle first and last points with dedicated function
    def handle_edge_points(points):
        threshold = np.percentile(points, threshold_percentile)
        peaks_over_threshold = points[points > threshold]
        shape, location, scale = stats.genpareto.fit(peaks_over_threshold)
        probs = 1 - stats.genpareto.cdf(points, shape, location, scale)
        return [1 if prob < 0.50 else 0 for prob in probs]

    anomaly_results.extend(handle_edge_points(scored['Loss_mae'].iloc[:window_size]))

    for i in range(window_size, len(scored['Loss_mae']) - window_size):
        window_data = scored['Loss_mae'].iloc[i-window_size:i+window_size+1]
        threshold = np.percentile(window_data, threshold_percentile)
        peaks_over_threshold = window_data[window_data > threshold]
        shape, location, scale = stats.genpareto.fit(peaks_over_threshold)
        probs = 1 - stats.genpareto.cdf(window_data, shape, location, scale)
        anomaly_results.append(1 if probs[window_size] < 0.50 else 0)

    anomaly_results.extend(handle_edge_points(scored['Loss_mae'].iloc[-window_size:]))

    return pd.DataFrame(anomaly_results, columns=['label'])

# Main processing starts here:

# Training Data Processing and MSE Calculation
train_pred = sequences_to_original(train_pred, T)
actual_train = clustered_dataframes[0].join(clustered_dataframes[1:])
actual_train2 = actual_train.values
train_mse = mean_squared_error(actual_train2, train_pred)
print(f"Mean Squared Error (Train): {train_mse}")

# Test Data Processing and MSE Calculation
X_test, test_data = process_data(file_paths['test'], cluster_labels, T)
test_pred = auto.predict(X_test)
test_pred = sequences_to_original(test_pred, T)
actual_test = test_data[actual_train.columns].values
test_mse = mean_squared_error(actual_test, test_pred)
print(f"Mean Squared Error (Test): {test_mse}")

# Anomaly Detection
scored = pd.DataFrame()
scored['Loss_mae'] = np.mean(np.abs(test_pred - actual_test), axis=1)
scored.to_csv('/home/Dataset/scored_attn.csv')

anomaly_results = dynamic_thresholding(scored)
anomaly_results.to_csv('/home/Dataset/anomaly_attn.csv')

# Evaluation
actual = load_data(file_paths['label'])
pred_pot = np.array(anomaly_results)
y_actual = actual.values

confusion_matrix = metrics.confusion_matrix(y_actual, pred_pot)
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=[True, False])

results_metrics = {
    'Train MSE': train_mse,
    'Test MSE': test_mse,
    'Accuracy POT': metrics.accuracy_score(y_actual, pred_pot),
    'Precision POT': metrics.precision_score(y_actual, pred_pot),
    'Recall POT': metrics.recall_score(y_actual, pred_pot),
    'F1-Score POT': metrics.f1_score(y_actual, pred_pot)
}

results = results.append(results_metrics, ignore_index=True)
results.to_csv('/home/Dataset/results_attn.csv', index=False)