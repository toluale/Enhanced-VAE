# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UGSOWQMnVC1IAdcyTs0TJEgBnKn1hg_V
"""

import tensorflow as tf
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import mean_squared_error
from tensorflow.keras.layers import (Input, LSTM, Dropout, RepeatVector, TimeDistributed, Dense,
                                    Concatenate, Lambda, MultiHeadAttention)
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping
from keras.optimizers import Adam
import tensorflow.keras.backend as K

# Global constants
T = 15
SEED = 40
N_CLUSTERS = 22
LEARNING_RATE = 0.001
EPOCHS = 60
BATCH_SIZE = 256

def configure_gpu():
    """Configure TensorFlow to use specified GPUs."""
    gpus = tf.config.experimental.list_physical_devices("GPU")
    tf.config.experimental.set_visible_devices([gpus[1], gpus[2]], "GPU")

def load_data(filepath):
    """Load data from CSV files."""
    return pd.read_csv(filepath, index_col=0)

def cluster_data(data, n_clusters):
    """Cluster data based on correlation and return clustered dataframes."""
    corr_mat = data.corr()
    clustering = KMeans(n_clusters=n_clusters, random_state=0).fit(corr_mat)
    cluster_labels = clustering.labels_

    clustered_dataframes = {}
    for label in np.unique(cluster_labels):
        columns_in_cluster = [col_name for col_name, col_cluster in zip(data.columns, cluster_labels) if col_cluster == label]
        clustered_dataframes[label] = data[columns_in_cluster]

    return clustered_dataframes

def create_sequences(data, seq_length):
    """Create input-output sequence pairs from the data."""
    xs = []
    ys = []

    for i in range(len(data)-seq_length):
        x = data[i:(i+seq_length)]
        y = data[i+seq_length]
        xs.append(x)
        ys.append(y)

    return np.array(xs), np.array(ys)

def sampling(args):
    z_mean, z_log_var = args
    batch = K.shape(z_mean)[0]
    dim = K.int_shape(z_mean)[1]
    epsilon = K.random_normal(shape=(batch, dim))
    return z_mean + K.exp(0.5 * z_log_var) * epsilon

def vae_model(X_list, kl_weight=1):
    """Create a VAE model."""
    inputs = []
    encoders = []
    total_features = sum([X.shape[2] for X in X_list])

    for X in X_list:
        inp = Input(shape=(X.shape[1], X.shape[2]))
        enc = LSTM(64, activation='relu', return_sequences=True)(inp)
        enc_attention = MultiHeadAttention(num_heads=2, key_dim=64)(enc, enc)
        enc = LSTM(32, activation='relu', return_sequences=False)(enc_attention)
        z_mean = Dense(32)(enc)
        z_log_var = Dense(32)(enc)
        z = Lambda(sampling, output_shape=(32,))([z_mean, z_log_var])
        inputs.append(inp)
        encoders.append(z)

    concat = Concatenate(axis=-1)(encoders)
    dec = RepeatVector(max([X.shape[1] for X in X_list]))(concat)
    dec = LSTM(64, activation='relu', return_sequences=True)(dec)
    dec_attention = MultiHeadAttention(num_heads=2, key_dim=64)(dec, dec)
    dec = LSTM(128, activation='relu', return_sequences=True)(dec_attention)
    output = TimeDistributed(Dense(total_features))(dec)

    vae = Model(inputs=inputs, outputs=output)

    kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)
    vae.add_loss(kl_weight * K.mean(kl_loss))
    vae.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='mse')

    return vae

def main():
    configure_gpu()
    strategy = tf.distribute.MirroredStrategy()

    with strategy.scope():
        file_paths = {
            "test": '/home/Dataset/test.csv',
            "train": '/home/Dataset/train.csv',
            "label": '/home/Dataset/label.csv'
        }

        train = load_data(file_paths['train'])
        clustered_dataframes = cluster_data(train, N_CLUSTERS)

        X = [create_sequences(clustered_dataframes[i].values, T)[0] for i in range(N_CLUSTERS)]
        auto = vae_model(X)

        total_features = sum([df.shape[2] for df in X])
        output_data = np.concatenate([df.reshape(df.shape[0], df.shape[1], df.shape[2]) for df in X], axis=-1)

        monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto', restore_best_weights=True)
        history = auto.fit(X, output_data, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.2, callbacks=[monitor], shuffle=False).history

        train_pred = auto.predict(X)

if __name__ == "__main__":
    main()